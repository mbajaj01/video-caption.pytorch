{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save video feats to data/feats/resnet152\n",
      "name:  data/train-video/v_-AjZCBMb4qU.mp4\n",
      "id:  v_-AjZCBMb4qU\n",
      " cleanup: resnet152_v_-AjZCBMb4qU/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pretrainedmodels\n",
    "from pretrainedmodels import utils\n",
    "\n",
    "C, H, W = 3, 224, 224\n",
    "\n",
    "\n",
    "def extract_frames(video, dst):\n",
    "    with open(os.devnull, \"w\") as ffmpeg_log:\n",
    "        if os.path.exists(dst):\n",
    "            print(\" cleanup: \" + dst + \"/\")\n",
    "            shutil.rmtree(dst)\n",
    "        os.makedirs(dst)\n",
    "        video_to_frames_command = [\"ffmpeg\",\n",
    "                                   # (optional) overwrite output file if it exists\n",
    "                                   '-y',\n",
    "                                   '-i', video,  # input file\n",
    "                                   '-vf', \"scale=400:300\",  # input file\n",
    "                                   '-qscale:v', \"2\",  # quality for JPEG\n",
    "                                   '{0}/%06d.jpg'.format(dst)]\n",
    "        subprocess.call(video_to_frames_command,\n",
    "                        stdout=ffmpeg_log, stderr=ffmpeg_log)\n",
    "\n",
    "        \n",
    "#Sample some frames from video and then convert to rgb and then pass through model to get features \n",
    "#and the save them.\n",
    "\n",
    "def extract_feats(params, model, load_image_fn):\n",
    "    global C, H, W\n",
    "    model.eval()\n",
    "\n",
    "    dir_fc = params['output_dir']\n",
    "    if not os.path.isdir(dir_fc):\n",
    "        os.mkdir(dir_fc)\n",
    "    print(\"save video feats to %s\" % (dir_fc))\n",
    "    video_list = glob.glob(os.path.join(params['video_path'], '*.mp4'))\n",
    "    for video in tqdm(video_list):\n",
    "        print(\"name: \",video)\n",
    "        video_id = video.split(\"/\")[-1].split(\".\")[0]\n",
    "        print(\"id: \",video_id)\n",
    "        dst = params['model'] + '_' + video_id\n",
    "        extract_frames(video, dst)\n",
    "\n",
    "        image_list = sorted(glob.glob(os.path.join(dst, '*.jpg')))\n",
    "        samples = np.round(np.linspace(\n",
    "            0, len(image_list) - 1, params['n_frame_steps']))\n",
    "        image_list = [image_list[int(sample)] for sample in samples]\n",
    "        images = torch.zeros((len(image_list), C, H, W))\n",
    "        for iImg in range(len(image_list)):\n",
    "            img = load_image_fn(image_list[iImg])\n",
    "            images[iImg] = img\n",
    "        with torch.no_grad():\n",
    "            fc_feats = model(images).squeeze()\n",
    "            #fc_feats = model(images.cuda()).squeeze()\n",
    "        img_feats = fc_feats.cpu().numpy()\n",
    "        # Save the inception features\n",
    "        outfile = os.path.join(dir_fc, video_id + '.npy')\n",
    "        np.save(outfile, img_feats)\n",
    "        # cleanup\n",
    "        shutil.rmtree(dst)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--gpu\", dest='gpu', type=str, default='0',\n",
    "                        help='Set CUDA_VISIBLE_DEVICES environment variable, optional')\n",
    "    parser.add_argument(\"--output_dir\", dest='output_dir', type=str,\n",
    "                        default='data/feats/resnet152', help='directory to store features')\n",
    "    parser.add_argument(\"--n_frame_steps\", dest='n_frame_steps', type=int, default=40,\n",
    "                        help='how many frames to sampler per video')\n",
    "\n",
    "    parser.add_argument(\"--video_path\", dest='video_path', type=str,\n",
    "                        default='data/train-video', help='path to video dataset')\n",
    "    parser.add_argument(\"--model\", dest=\"model\", type=str, default='resnet152',\n",
    "                        help='the CNN model you want to use to extract_feats')\n",
    "    \n",
    "    import sys\n",
    "    sys.argv=['']\n",
    "    del sys\n",
    "    args = parser.parse_args()\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "    params = vars(args)\n",
    "    if params['model'] == 'inception_v3':\n",
    "        C, H, W = 3, 299, 299\n",
    "        model = pretrainedmodels.inceptionv3(pretrained='imagenet')\n",
    "        load_image_fn = utils.LoadTransformImage(model)\n",
    "\n",
    "    elif params['model'] == 'resnet152':\n",
    "        C, H, W = 3, 224, 224\n",
    "        model = pretrainedmodels.resnet152(pretrained='imagenet')\n",
    "        load_image_fn = utils.LoadTransformImage(model)\n",
    "\n",
    "    elif params['model'] == 'inception_v4':\n",
    "        C, H, W = 3, 299, 299\n",
    "        model = pretrainedmodels.inceptionv4(\n",
    "            num_classes=1000, pretrained='imagenet')\n",
    "        load_image_fn = utils.LoadTransformImage(model)\n",
    "\n",
    "    else:\n",
    "        print(\"doesn't support %s\" % (params['model']))\n",
    "\n",
    "    model.last_linear = utils.Identity()\n",
    "    #model = nn.DataParallel(model)\n",
    "    \n",
    "    #model = model.cuda()\n",
    "    extract_feats(params, model, load_image_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.load('data/feats/resnet152/v_-AjZCBMb4qU.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_vocab(vids, params):\n",
    "    count_thr = params['word_count_threshold']\n",
    "    # count up the number of words\n",
    "    counts = {}\n",
    "    for vid, caps in vids.items():\n",
    "        for cap in caps['captions']:\n",
    "            ws = re.sub(r'[.!,;?]', ' ', cap).split()\n",
    "            for w in ws:\n",
    "                counts[w] = counts.get(w, 0) + 1\n",
    "    # cw = sorted([(count, w) for w, count in counts.items()], reverse=True)\n",
    "    total_words = sum(counts.values())\n",
    "    bad_words = [w for w, n in counts.items() if n <= count_thr]\n",
    "    vocab = [w for w, n in counts.items() if n > count_thr]\n",
    "    bad_count = sum(counts[w] for w in bad_words)\n",
    "    print('number of bad words: %d/%d = %.2f%%' %\n",
    "          (len(bad_words), len(counts), len(bad_words) * 100.0 / len(counts)))\n",
    "    print('number of words in vocab would be %d' % (len(vocab), ))\n",
    "    print('number of UNKs: %d/%d = %.2f%%' %\n",
    "          (bad_count, total_words, bad_count * 100.0 / total_words))\n",
    "    # lets now produce the final annotations\n",
    "    if bad_count > 0:\n",
    "        # additional special UNK token we will use below to map infrequent words to\n",
    "        print('inserting the special UNK token')\n",
    "        vocab.append('<UNK>')\n",
    "    for vid, caps in vids.items():\n",
    "        caps = caps['captions']\n",
    "        vids[vid]['final_captions'] = []\n",
    "        for cap in caps:\n",
    "            ws = re.sub(r'[.!,;?]', ' ', cap).split()\n",
    "            caption = [\n",
    "                '<sos>'] + [w if counts.get(w, 0) > count_thr else '<UNK>' for w in ws] + ['<eos>']\n",
    "            vids[vid]['final_captions'].append(caption)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    videos = json.load(open(params['input_json'], 'r'))['sentences']\n",
    "    video_caption = {}\n",
    "    for i in videos:\n",
    "        if i['video_id'] not in video_caption.keys():\n",
    "            video_caption[i['video_id']] = {'captions': []}\n",
    "        video_caption[i['video_id']]['captions'].append(i['caption'])\n",
    "    # create the vocab\n",
    "    vocab = build_vocab(video_caption, params)\n",
    "    itow = {i + 2: w for i, w in enumerate(vocab)}\n",
    "    wtoi = {w: i + 2 for i, w in enumerate(vocab)}  # inverse table\n",
    "    wtoi['<eos>'] = 0\n",
    "    itow[0] = '<eos>'\n",
    "    wtoi['<sos>'] = 1\n",
    "    itow[1] = '<sos>'\n",
    "\n",
    "    out = {}\n",
    "    out['ix_to_word'] = itow\n",
    "    out['word_to_ix'] = wtoi\n",
    "    out['videos'] = {'train': [], 'val': [], 'test': []}\n",
    "    videos = json.load(open(params['input_json'], 'r'))['videos']\n",
    "    for i in videos:\n",
    "        out['videos'][i['split']].append(int(i['id']))\n",
    "    json.dump(out, open(params['info_json'], 'w'))\n",
    "    json.dump(video_caption, open(params['caption_json'], 'w'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # input json\n",
    "    parser.add_argument('--input_json', type=str, default='data/videodatainfo_2017.json',\n",
    "                        help='msr_vtt videoinfo json')\n",
    "    parser.add_argument('--info_json', default='data/info.json',\n",
    "                        help='info about iw2word and word2ix')\n",
    "    parser.add_argument('--caption_json', default='data/caption.json', help='caption json file')\n",
    "\n",
    "\n",
    "    parser.add_argument('--word_count_threshold', default=1, type=int,\n",
    "                        help='only words that occur more than this number of times will be put in vocab')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    params = vars(args)  # convert to ordinary dict\n",
    "    main(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
